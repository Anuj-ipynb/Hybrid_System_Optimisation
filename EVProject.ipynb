{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuj-ipynb/Hybrid_System_Optimisation/blob/main/Untitled46.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1KXmUXiQzvXI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridElectricEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(HybridElectricEnv, self).__init__()\n",
        "\n",
        "        self.action_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([1, 1]), dtype=np.float32)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=np.array([0, 0, 0]), high=np.array([1, 1, 1]), dtype=np.float32\n",
        "        )\n",
        "        self.state = np.random.uniform(0, 1, size=(3,))\n",
        "        self.noise_reduction_factor = 0.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.uniform(0, 1, size=(3,))\n",
        "        self.noise_reduction_factor = 0.0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        turbine_power, motor_power = action\n",
        "        vibration = self.state[0] - 0.1 * turbine_power\n",
        "        noise = self.state[1] - 0.2 * motor_power\n",
        "        energy_efficiency = self.state[2] + 0.05 * motor_power - 0.03 * turbine_power\n",
        "        self.state = np.clip([vibration, noise, energy_efficiency], 0, 1)\n",
        "\n",
        "        reward = (\n",
        "            -abs(noise - 0.2)\n",
        "            + energy_efficiency\n",
        "            - abs(turbine_power + motor_power - 1)\n",
        "        )\n",
        "\n",
        "        done = False\n",
        "        return self.state, reward, done, {}"
      ],
      "metadata": {
        "id": "ZisQYJCnANpf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.shape[0]\n",
        "        self.actor_model = self.build_actor()\n",
        "        self.critic_model = self.build_critic()\n",
        "        self.total_rewards = []\n",
        "\n",
        "    def build_actor(self):\n",
        "        inputs = layers.Input(shape=(self.state_size,))\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(self.action_size, activation=\"sigmoid\")(x)\n",
        "        model = models.Model(inputs, outputs)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
        "        return model\n",
        "\n",
        "    def build_critic(self):\n",
        "        inputs = layers.Input(shape=(self.state_size,))\n",
        "        x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        outputs = layers.Dense(1, activation=\"linear\")(x)\n",
        "        model = models.Model(inputs, outputs)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=\"mse\")\n",
        "        return model\n",
        "\n",
        "    def train(self, episodes=20):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "\n",
        "            for step in range(200):\n",
        "                state = state.reshape(1, -1)\n",
        "                action = self.actor_model.predict(state, verbose=0)[0]\n",
        "                action = np.clip(action, 0, 1)\n",
        "\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                total_reward += reward\n",
        "\n",
        "                target = reward + 0.99 * self.critic_model.predict(next_state.reshape(1, -1), verbose=0)\n",
        "                self.critic_model.train_on_batch(state, target)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    advantages = target - self.critic_model.predict(state, verbose=0)\n",
        "                    action_probs = self.actor_model(state, training=True)\n",
        "                    loss = -tf.reduce_mean(advantages * tf.math.log(action_probs))\n",
        "                grads = tape.gradient(loss, self.actor_model.trainable_variables)\n",
        "                self.actor_model.optimizer.apply_gradients(zip(grads, self.actor_model.trainable_variables))\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            self.total_rewards.append(total_reward)\n",
        "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    def plot_rewards(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.lineplot(x=range(1, len(self.total_rewards) + 1), y=self.total_rewards, marker=\"o\")\n",
        "        plt.title(\"Total Rewards per Episode\", fontsize=16)\n",
        "        plt.xlabel(\"Episode\", fontsize=14)\n",
        "        plt.ylabel(\"Total Reward\", fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def predict_new_episode(self):\n",
        "        state = self.env.reset()\n",
        "        action = self.actor_model.predict(state.reshape(1, -1), verbose=0)[0]\n",
        "        action = np.clip(action, 0, 1)\n",
        "        print(f\"Predicted Action for New Episode: {action}\")\n",
        "        next_state, _, _, _ = self.env.step(action)\n",
        "        print(f\"Predicted Next State: {next_state}\")\n",
        "        return next_state\n",
        "\n",
        "\n",
        "def train_linear_regression_model():\n",
        "    state_data = np.random.random((1000, 3))\n",
        "    reward_data = np.random.random(1000)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    state_data_scaled = scaler.fit_transform(state_data)\n",
        "\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(state_data_scaled, reward_data)\n",
        "\n",
        "\n",
        "    predictions = model.predict(state_data_scaled)\n",
        "    mse = mean_squared_error(reward_data, predictions)\n",
        "    r2 = r2_score(reward_data, predictions)\n",
        "    evs = explained_variance_score(reward_data, predictions)\n",
        "\n",
        "    print(f\"Linear Regression Model R-squared: {r2:.4f}\")\n",
        "    print(f\"Linear Regression Model MSE: {mse:.4f}\")\n",
        "    print(f\"Linear Regression Model Explained Variance Score: {evs:.4f}\")\n",
        "\n",
        "\n",
        "    accuracy = evs * 100\n",
        "    print(f\"Linear Regression Model 'Accuracy' (Explained Variance  * 100): {accuracy:.2f}%\")\n",
        ""
      ],
      "metadata": {
        "id": "meMDvP3Q6uY7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = HybridElectricEnv()\n",
        "    agent = PPOAgent(env)\n",
        "    agent.train(episodes=20)\n",
        "    agent.plot_rewards()\n",
        "    agent.predict_new_episode()\n",
        "    train_linear_regression_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcMKBpOkAbqs",
        "outputId": "97bc6939-5c41-4e32-ec10-4f22b92b9c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:315: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  return float(self._numpy())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 25.836630580402353\n"
          ]
        }
      ]
    }
  ]
}
